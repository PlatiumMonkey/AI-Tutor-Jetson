# File: src/core/llm_engine.py
import requests
import json

class LLMEngine:
    def __init__(self):
        # Äá»‹a chá»‰ cá»§a Ollama trong máº¡ng ná»™i bá»™ Docker (xem docker-compose.yaml)
        self.ollama_url = "http://ollama:11434/api/generate"
        
        # TÃªn model Ä‘Ã£ táº£i (Ä‘áº£m báº£o khá»›p vá»›i cÃ¡i báº¡n Ä‘Ã£ pull)
        self.model_name = "llama3.2:3b"
        
        # Cáº¤U HÃŒNH Tá»I Æ¯U CHO JETSON (Giai Ä‘oáº¡n 3.1)
        # temperature: 0.7 (SÃ¡ng táº¡o vá»«a Ä‘á»§, khÃ´ng bá»‹a Ä‘áº·t)
        # top_k: 40 (Lá»c bá»›t cÃ¡c tá»« xÃ¡c suáº¥t tháº¥p giÃºp cháº¡y nhanh hÆ¡n)
        # num_ctx: 2048 (Äá»™ dÃ i ngá»¯ cáº£nh, giáº£m xuá»‘ng náº¿u trÃ n RAM)
        self.config = {
            "temperature": 0.7,
            "top_k": 40,
            "top_p": 0.9,
            "num_ctx": 2048, 
            "num_predict": 512, # Giá»›i háº¡n sá»‘ tá»« tráº£ lá»i Ä‘á»ƒ khÃ´ng chá» lÃ¢u
            "repeat_penalty": 1.1 # TrÃ¡nh láº·p tá»«
        }

    def generate_response(self, prompt: str, system_prompt: str = None):
        """
        Gá»­i yÃªu cáº§u sang Ollama vÃ  nháº­n pháº£n há»“i streaming hoáº·c full text
        """
        try:
            # Chuáº©n bá»‹ payload
            final_prompt = prompt
            if system_prompt:
                # GhÃ©p vai trÃ² (System Prompt) vÃ o Ä‘áº§u
                final_prompt = f"<|system|>\n{system_prompt}\n<|user|>\n{prompt}\n<|assistant|>"

            payload = {
                "model": self.model_name,
                "prompt": final_prompt,
                "stream": False, # Táº¡m thá»i táº¯t stream Ä‘á»ƒ dá»… debug
                "options": self.config
            }

            print(f"ğŸ¤– Äang gá»­i yÃªu cáº§u tá»›i {self.model_name}...")
            
            # Gá»i API
            response = requests.post(self.ollama_url, json=payload)
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "")
            else:
                return f"Lá»—i Ollama: {response.status_code} - {response.text}"

        except Exception as e:
            return f"Lá»—i káº¿t ná»‘i LLM Engine: {str(e)}"

# Test cháº¡y Ä‘á»™c láº­p
if __name__ == "__main__":
    llm = LLMEngine()
    print(llm.generate_response("Hello, are you ready?"))